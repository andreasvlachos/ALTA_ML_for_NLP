{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "brown_words = brown.words()\n",
    "unigram_counts = Counter(brown_words)\n",
    "bigrams = []\n",
    "for sent in brown.sents():\n",
    "    bigrams.extend(nltk.bigrams(sent, pad_left=True, pad_right=True))\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "def bigram_LM(sentence_x, smoothing=0.0):\n",
    "    unique_words = len(unigram_counts.keys()) + 2 # For the None paddings\n",
    "    x_bigrams = nltk.bigrams(sentence_x, pad_left=True, pad_right=True)\n",
    "    prob_x = 1.0\n",
    "    for bg in x_bigrams:\n",
    "        if bg[0] == None:\n",
    "            prob_bg = (bigram_counts[bg]+smoothing)/(len(brown.sents())+smoothing*unique_words)\n",
    "        else:\n",
    "            prob_bg = (bigram_counts[bg]+smoothing)/(unigram_counts[bg[0]]+smoothing*unique_words)\n",
    "        prob_x = prob_x *prob_bg\n",
    "        print(str(bg)+\":\"+str(prob_bg))\n",
    "    return prob_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Language modeling</h3>\n",
    "\n",
    "Another(!) very common problem in NLP:\n",
    "<center>\n",
    "<p style=\"border:3px; width: 75%; border-radius: 25px; background-color:lightgrey; border-style:solid; border-color:black; padding: 1em;\">\n",
    "<i>how likely is that a sequence of words comes from a particular language (e.g. English)?\n",
    "</i>\n",
    "</p>\n",
    "</center>\n",
    "\n",
    "<b>Odd problem. Applications?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- speech recognition\n",
    "- machine translation\n",
    "- grammatical error detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language modeling with the perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maybe:\n",
    "- we have a lot of data: all the English Web\n",
    "- we don't have labels\n",
    "- can assume everything we see is \"good English\"; negative instances?\n",
    "- we need probabilities; less probable not ungrammatical\n",
    "\n",
    "Let't think about it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Problem setup</h3>\n",
    "\n",
    "Training data is a (large) set of sentences $\\mathbf{x}^m$ with words $x_n$:\n",
    "\n",
    "<p>\n",
    "\\begin{align}\n",
    "D_{train} & = \\{\\mathbf{x}^1,...,\\mathbf{x}^M\\} \\\\\n",
    "\\mathbf{x}& = [x_1,... x_N]\\\\\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "<p class=\"fragment\">\n",
    "for example:\n",
    "\\begin{align}\n",
    "\\mathbf{x}=&[\\text{The}, \\text{water}, \\text{is}, \\text{clear}, \\text{.}]\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "<p class=\"fragment\">We want to learn a model that returns:\n",
    "\\begin{align}\n",
    "\\text{probability}\\; P(\\mathbf{x}), \\mathbf{for} \\; \\forall \\mathbf{x}\\in V^{maxN}\n",
    "\\end{align}\n",
    "$V$ is the vocabulary and $V^{maxN}$ all possible sentences\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's take a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "brown.sents() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Count its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Count the occurrences of one word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(unigram_counts[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word counts to probabilities\n",
    "\n",
    "probability $P(\\text{the}) = \\frac{counts(the)}{\\sum_{x \\in Vocabulary} counts(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(unigram_counts[\"the\"]/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To have a probability distributions and not just scores, the sum of the probabilities for all words must be 1. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for word in unigram_counts:\n",
    "    sum += unigram_counts[word]\n",
    "print(sum/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unigram language model\n",
    "\n",
    "Multiply the probability of each word in the sentence $\\mathbf{x}$:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N  P(x_n) = \\prod_{n=1}^N \\frac{counts(x_n)}{\\sum_{x \\in V} counts(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our first language model! What could go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the most probable word is \"the\"\n",
    "- the most probable single-word sentence is \"the\"\n",
    "- the most probable two-word sentence is \"the the\"\n",
    "- the most probable $N$-word sentence is $N\\times $\"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sentences with words not seen in the training data have 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(unigram_counts[\"genome\"]/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>On the way to a better language model</h3>\n",
    "\n",
    "Instead of:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N  P(x_n)$$\n",
    "\n",
    "Let's assume that the choice of a word depends on the one before it:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N P(x_n| x_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of a word $x_n$ following another $x_{n-1}$ is:\n",
    "\n",
    "$$P(x_n| x_{n-1})=\\frac{bigram\\_counts(x_{n-1}, x_n)}{counts(x_{n-1})}$$\n",
    "\n",
    "This is the **bigram** language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram language model in action\n",
    "\n",
    "$$\\mathbf{x}=[\\text{The}, \\text{water}, \\text{is}, \\text{clear}, \\text{.}]$$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{x}) =& P(\\text{The}|\\text{None})\\times P(\\text{water}|\\text{The})\\times P\\text{is}|\\text{water})\\times\\\\\n",
    "&P(\\text{clear}|\\text{is})\\times P(\\text{.}|\\text{clear})\\times P\\text{.}|\\text{None})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = [\"The\", \"water\", \"is\", \"clear\", \".\"]\n",
    "print(bigram_LM(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Longer contexts\n",
    "\n",
    "$$P(x|context) = \\frac{P(context,x)}{P(context)} = \\frac{counts(context,x)}{counts(context)}  $$\n",
    "\n",
    "In bigram LM $context$ is $x_{n-1}$, trigram $x_{n-2}, x_{n-1}$, etc.\n",
    "\n",
    "The longer the context:\n",
    "- the more likely to capture long-range dependencies: \"<i>I saw a tiger that was really very...</i>\" <b>fierce</b> or <b>talkative</b>?\n",
    "- the sparser the counts (zero probabilities)\n",
    "\n",
    "5-grams\n",
    "and <a href=\"http://googleresearch.blogspot.co.uk/2006/08/all-our-n-gram-are-belong-to-you.html\">\n",
    "training sets with billions of tokens</a>  are common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about the unseen words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img style:\"float:left\" src=\"images/Fairbanks_Robin_Hood_standing_by_wall_w_sword.jpg\" />\n",
    "    \n",
    "    \n",
    "<p>... stealing from the rich and giving to the poor!</p>\n",
    "\n",
    "<p style=\"font-size:50%\"><a href=\"https://en.wikipedia.org/wiki/Robin_Hood\"></a>\n",
    "By United Artists, cinematographers Arthur Edeson &amp; Charles Richardson - <a rel=\"nofollow\" class=\"external text\" href=\"http://douglasfairbanks.org/robin1.jpg\">Here</a>, Public Domain, <a href=\"https://commons.wikimedia.org/w/index.php?curid=819264\">Link</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Smoothing intuition</h3>\n",
    "\n",
    "<a href=\"http://www.cs.berkeley.edu/~klein/cs288/sp10/slides/SP10%20cs288%20lecture%202%20--%20language%20models%20(2PP).pdf\"><img width=\"80%\" src=\"images/smoothing.png\"></a>\n",
    "<p>Taking from the frequent and giving to the rare (discounting)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Smoothing\n",
    "\n",
    "Pretend we have seen everything at least once:\n",
    "\n",
    "$$P_{add-1}(x_n|x_{n-1}) = \\frac{counts(x_{n-1},x_n)+1}{counts(x_{n-1}) + |V|}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Add-1 puts too much probability mass to unseen bigrams, better to add-$k, k<1$:\n",
    "$$P_{add-k}(x_n|x_{n-1}) = \\frac{counts(x_{n-1},x_{n})+k}{counts(x_{n-1}) + k|V|}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Smoothing in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = [\"The\", \"water\", \"is\", \"crystal\", \"clear\", \".\"]\n",
    "print(bigram_LM(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(bigram_LM(x, smoothing=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More smoothing\n",
    "\n",
    "If a word was never encountered in training,\n",
    "any sentence containing will have probability 0\n",
    "\n",
    "It happens:\n",
    "- all corpora are finite\n",
    "- new words emerging\n",
    "\n",
    "\n",
    "Common solutions: \n",
    "- Generate unknown words in the training data by\n",
    "replacing low-frequency words with a special UNKNOWN word to represent them\n",
    "- Use classes of unknown words, e.g. names and numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation \n",
    "\n",
    "**Intrinsic** evaluation for language modeling\n",
    "\n",
    "How well does our model predict the next word?\n",
    "\n",
    "*I always order pizza with cheese and...*\n",
    "- mushrooms?\n",
    "- bread?\n",
    "- and?\n",
    "\n",
    "Measure accuracy/perplexity (how surprised is the LM by the correct word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation \n",
    "\n",
    "**Extrinsic** evaluation for language modeling\n",
    "\n",
    "- Grammatical error correction: detecting \"odd\" sentences and propose alternatives\n",
    "- Natural lanuage generation: prefer more \"natural\" sentences\n",
    "- Machine translation, speech recognition"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
