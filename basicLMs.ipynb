{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "brown_words = brown.words()\n",
    "unigram_counts = Counter(brown_words)\n",
    "bigrams = []\n",
    "for sent in brown.sents():\n",
    "    bigrams.extend(nltk.bigrams(sent, pad_left=True, pad_right=True))\n",
    "bigram_counts = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Language modeling</h3>\n",
    "\n",
    "Another(!) very common problem in NLP:\n",
    "<center>\n",
    "<p style=\"border:3px; width: 75%; border-radius: 25px; background-color:lightgrey; border-style:solid; border-color:black; padding: 1em;\">\n",
    "<i>how likely is that a sequence of words comes from a particular language (e.g. English)?\n",
    "</i>\n",
    "</p>\n",
    "</center>\n",
    "\n",
    "<b>Odd problem. Applications?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- speech recognition\n",
    "- machine translation\n",
    "- grammatical error detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language modeling with the perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maybe:\n",
    "- we have a lot of data: all the English Web\n",
    "- we don't have labels\n",
    "- can assume everything we see is \"good English\"; negative instances?\n",
    "- we need probabilities; less probable not ungrammatical\n",
    "\n",
    "Let't think about it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Problem setup</h3>\n",
    "\n",
    "Training data is a (large) set of sentences $\\mathbf{x}^m$ with words $x_n$:\n",
    "\n",
    "<p>\n",
    "\\begin{align}\n",
    "D_{train} & = \\{\\mathbf{x}^1,...,\\mathbf{x}^M\\} \\\\\n",
    "\\mathbf{x}& = [x_1,... x_N]\\\\\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "<p class=\"fragment\">\n",
    "for example:\n",
    "\\begin{align}\n",
    "\\mathbf{x}=&[\\text{The}, \\text{water}, \\text{is}, \\text{clear}, \\text{.}]\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "<p class=\"fragment\">We want to learn a model that returns:\n",
    "\\begin{align}\n",
    "\\text{probability}\\; P(\\mathbf{x}), \\mathbf{for} \\; \\forall \\mathbf{x}\\in V^{maxN}\n",
    "\\end{align}\n",
    "$V$ is the vocabulary and $V^{maxN}$ all possible sentences\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's take a corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Count its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161192\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Count the occurrences of one word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62713\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counts[\"the\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word counts to probabilities\n",
    "\n",
    "probability $P(\\text{the}) = \\frac{counts(the)}{\\sum_{x \\in Vocabulary} counts(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05400743374050114\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counts[\"the\"]/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To have a probability distributions and not just scores, the sum of the probabilities for all words must be 1. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for word in unigram_counts:\n",
    "    sum += unigram_counts[word]\n",
    "print(sum/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unigram language model\n",
    "\n",
    "Multiply the probability of each word in the sentence $\\mathbf{x}$:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N  P(x_n) = \\prod_{n=1}^N \\frac{counts(x_n)}{\\sum_{x \\in V} counts(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our first language model! What could go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "the most probable word is \"the\"\n",
    "- the most probable single-word sentence is \"the\"\n",
    "- the most probable two-word sentence is \"the the\"\n",
    "- the most probable $N$-word sentence is $N\\times $\"the\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sentences with words not seen in the training data have 0 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(unigram_counts[\"genome\"]/len(brown_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>On the way to a better language model</h3>\n",
    "\n",
    "Instead of:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N  P(x_n)$$\n",
    "\n",
    "Let's assume that the choice of a word depends on the one before it:\n",
    "\n",
    "$$P(\\mathbf{x}) = \\prod_{n=1}^N P(x_n| x_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The probability of a word $x_n$ following another $x_{n-1}$ is:\n",
    "\n",
    "$$P(x_n| x_{n-1})=\\frac{bigram\\_counts(x_{n-1}, x_n)}{counts(x_{n-1})}$$\n",
    "\n",
    "This is the **bigram** language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bigram Language model in action\n",
    "\n",
    "$$\\mathbf{x}=[\\text{The}, \\text{water}, \\text{is}, \\text{clear}, \\text{.}]$$\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{x}) =& P(\\text{The}|\\text{None})\\times P(\\text{water}|\\text{The})\\times P\\text{is}|\\text{water})\\times\\\\\n",
    "&P(\\text{clear}|\\text{is})\\times P(\\text{.}|\\text{clear})\\times P\\text{.}|\\text{None})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 'The'):0.11412626438786187\n",
      "('The', 'water'):0.0009644530173601543\n",
      "('water', 'is'):0.030162412993039442\n",
      "('is', 'clear'):0.002497253021676156\n",
      "('clear', '.'):0.091324200913242\n",
      "('.', None):1.0\n",
      "7.571487129955549e-10\n"
     ]
    }
   ],
   "source": [
    "x = [\"The\", \"water\", \"is\", \"clear\", \".\"]\n",
    "x_bigrams = nltk.bigrams(x, pad_left=True, pad_right=True)\n",
    "prob_x = 1.0\n",
    "for bg in x_bigrams:\n",
    "    if bg[0] == None:\n",
    "        prob_bg = bigram_counts[bg]/len(brown.sents())\n",
    "    else:\n",
    "        prob_bg = bigram_counts[bg]/unigram_counts[bg[0]]\n",
    "    prob_x = prob_x *prob_bg\n",
    "    print(str(bg)+\":\"+str(prob_bg))\n",
    "print(prob_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order n-gram language models are possible and are used with very large datasets (5-grams with 1B tokens are common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation \n",
    "there is perplexity but...\n",
    "with sentence completion/error correction"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
